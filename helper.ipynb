{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 22 00:25:07 2020\n",
    "\n",
    "@author: hp\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import spacy\n",
    "import math\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from spellchecker import SpellChecker\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "new_words = set([i.lower() for i in ['sprintcare', 'Ask_Spectrum', 'VerizonSupport', 'ChipotleTweets',\n",
    "       'AskPlayStation', 'marksandspencer', 'MicrosoftHelps',\n",
    "       'ATVIAssist', 'AdobeCare', 'AmazonHelp', 'XboxSupport',\n",
    "       'AirbnbHelp', 'AirAsiaSupport', 'Morrisons', 'NikeSupport',\n",
    "       'AskAmex', 'YahooCare', 'AskLyft', 'UPSHelp', 'Delta', 'McDonalds',\n",
    "       'AppleSupport', 'Uber_Support', 'Tesco', 'SpotifyCares',\n",
    "       'British_Airways', 'comcastcares', 'AmericanAir', 'TMobileHelp',\n",
    "       'VirginTrains', 'SouthwestAir', 'AskeBay', 'hulu_support',\n",
    "       'GWRHelp', 'sainsburys', 'AskPayPal', 'HPSupport', 'ChaseSupport',\n",
    "       'CoxHelp', 'DropboxSupport', 'VirginAtlantic', 'BofA_Help',\n",
    "       'AzureSupport', 'AlaskaAir', 'ArgosHelpers', 'Postmates_Help',\n",
    "       'AskTarget', 'GoDaddyHelp', 'CenturyLinkHelp', 'AskPapaJohns',\n",
    "       'SW_Help', 'nationalrailenq', 'askpanera', 'Walmart',\n",
    "       'USCellularCares', 'AsurionCares', 'GloCare', 'idea_cares',\n",
    "       'DoorDash_Help', 'NeweggService', 'VirginAmerica',\n",
    "       'Ask_WellsFargo', 'O2', 'asksalesforce', 'airtel_care', 'Kimpton',\n",
    "       'AskCiti', 'IHGService', 'JetBlue', 'BoostCare', 'JackBox',\n",
    "       'HiltonHelp', 'GooglePlayMusic', 'KFC_UKI_Help', 'DellCares',\n",
    "       'TwitterSupport', 'GreggsOfficial', 'LondonMidland', 'ATT',\n",
    "       'TacoBellTeam', 'Safaricom_Care', 'AskRBC', 'ArbysCares',\n",
    "       'NortonSupport', 'AskSeagate', 'sizehelpteam', 'TfL', 'AldiUK',\n",
    "       'SCsupport', 'AskDSC', 'AskVirginMoney', 'AskRobinhood',\n",
    "       'MTNC_Care', 'DunkinDonuts', 'AWSSupport', 'VMUcare',\n",
    "       'mediatemplehelp', 'MOO', 'PandoraSupport', 'askvisa',\n",
    "       'OPPOCareIN', 'ask_progressive', 'PearsonSupport', 'AskTigogh',\n",
    "       'OfficeSupport', 'CarlsJr', 'HotelTonightCX', 'KeyBank_Help']])\n",
    "sw.update(new_words)\n",
    "\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Provided by https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "EMOTICONS = {\n",
    "    u\":‑\\)\":\"Happy face or smiley\",\n",
    "    u\":\\)\":\"Happy face or smiley\",\n",
    "    u\":-\\]\":\"Happy face or smiley\",\n",
    "    u\":\\]\":\"Happy face or smiley\",\n",
    "    u\":-3\":\"Happy face smiley\",\n",
    "    u\":3\":\"Happy face smiley\",\n",
    "    u\":->\":\"Happy face smiley\",\n",
    "    u\":>\":\"Happy face smiley\",\n",
    "    u\"8-\\)\":\"Happy face smiley\",\n",
    "    u\":o\\)\":\"Happy face smiley\",\n",
    "    u\":-\\}\":\"Happy face smiley\",\n",
    "    u\":\\}\":\"Happy face smiley\",\n",
    "    u\":-\\)\":\"Happy face smiley\",\n",
    "    u\":c\\)\":\"Happy face smiley\",\n",
    "    u\":\\^\\)\":\"Happy face smiley\",\n",
    "    u\"=\\]\":\"Happy face smiley\",\n",
    "    u\"=\\)\":\"Happy face smiley\",\n",
    "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\":\"Very happy\",\n",
    "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
    "    u\":@\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":'‑\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‑\\)\":\"Tears of happiness\",\n",
    "    u\":'\\)\":\"Tears of happiness\",\n",
    "    u\"D‑':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"Great dismay\",\n",
    "    u\"D;\":\"Great dismay\",\n",
    "    u\"D=\":\"Great dismay\",\n",
    "    u\"DX\":\"Great dismay\",\n",
    "    u\":‑O\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‑o\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‑0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‑\\)\":\"Wink or smirk\",\n",
    "    u\";\\)\":\"Wink or smirk\",\n",
    "    u\"\\*-\\)\":\"Wink or smirk\",\n",
    "    u\"\\*\\)\":\"Wink or smirk\",\n",
    "    u\";‑\\]\":\"Wink or smirk\",\n",
    "    u\";\\]\":\"Wink or smirk\",\n",
    "    u\";\\^\\)\":\"Wink or smirk\",\n",
    "    u\":‑,\":\"Wink or smirk\",\n",
    "    u\";D\":\"Wink or smirk\",\n",
    "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‑\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed or blushing\",\n",
    "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
    "    u\"0:3\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
    "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
    "    u\">:‑\\)\":\"Evil or devilish\",\n",
    "    u\">:\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:\\)\":\"Evil or devilish\",\n",
    "    u\"3:‑\\)\":\"Evil or devilish\",\n",
    "    u\"3:\\)\":\"Evil or devilish\",\n",
    "    u\">;\\)\":\"Evil or devilish\",\n",
    "    u\"\\|;‑\\)\":\"Cool\",\n",
    "    u\"\\|‑O\":\"Bored\",\n",
    "    u\":‑J\":\"Tongue-in-cheek\",\n",
    "    u\"#‑\\)\":\"Party all night\",\n",
    "    u\"%‑\\)\":\"Drunk or confused\",\n",
    "    u\"%\\)\":\"Drunk or confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‑\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;_;\":\"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
    "    u\";_;\":\"Sad or Crying\",\n",
    "    u\";-;\":\"Sad or Crying\",\n",
    "    u\";n;\":\"Sad or Crying\",\n",
    "    u\";;\":\"Sad or Crying\",\n",
    "    u\"Q\\.Q\":\"Sad or Crying\",\n",
    "    u\"T\\.T\":\"Sad or Crying\",\n",
    "    u\"QQ\":\"Sad or Crying\",\n",
    "    u\"Q_Q\":\"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(一一\\)\":\"Shame\",\n",
    "    u\"\\(；一_一\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\・\\・?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
    "    u\"\\^/\\^\":\"Normal Laugh\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
    "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(ーー;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
    "}\n",
    "\n",
    "# Provided by https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "text_speech = \"\"\"\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "QPSA?\tQue Pasa?\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "SMH = Shake My Head\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "\"\"\"\n",
    "\n",
    "chat_dict = {}\n",
    "chat_list = []\n",
    "for i in text_speech.split(\"\\n\"):\n",
    "    if i != \"\":\n",
    "        tw = i.split(\"=\")[0]\n",
    "        full_text = i.split(\"=\")[-1]\n",
    "        chat_list.append(tw)\n",
    "        chat_dict[tw] = full_text\n",
    "            \n",
    "chat_list = set(chat_list)\n",
    "\n",
    "def day_of_week_num(dts):\n",
    "    '''\n",
    "    weekday: takes in the day_of_week_num converted column\n",
    "    and assigns those numbers (after looking up which day\n",
    "    of the week 0 landed on) and assigned a string representation\n",
    "    of that day of the week\n",
    "    Parameters\n",
    "    ----------\n",
    "    dts: Integer\n",
    "    Returns\n",
    "    -------\n",
    "    dts: Python string of the day of the week\n",
    "    '''\n",
    "    return (dts.view('int64') - 4) % 7\n",
    "\n",
    "def weekday(dts):\n",
    "    '''\n",
    "    weekday: takes in the day_of_week_num converted column\n",
    "    and assigns those numbers (after looking up which day\n",
    "    of the week 0 landed on) and assigned a string representation\n",
    "    of that day of the week\n",
    "    Parameters\n",
    "    ----------\n",
    "    dts: Integer\n",
    "    Returns\n",
    "    -------\n",
    "    dts: Python string of the day of the week\n",
    "    '''\n",
    "    if dts == 0:\n",
    "        dts = 'Tuesday'\n",
    "    elif dts == 1:\n",
    "        dts = 'Wednesday'\n",
    "    elif dts == 2:\n",
    "        dts = 'Thursday'\n",
    "    elif dts == 3:\n",
    "        dts = 'Friday'\n",
    "    elif dts == 4:\n",
    "        dts = 'Saturday'\n",
    "    elif dts == 5:\n",
    "        dts = 'Sunday'\n",
    "    else:\n",
    "        dts = 'Monday'\n",
    "    return dts\n",
    "\n",
    "def set_response_category(col):\n",
    "    '''\n",
    "    set_response_category: takes in the minutes to respond\n",
    "    and sets a cutoff point between less than or equal to 27\n",
    "    as \"Fast\" and above 27 as \"Slow\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Numpy Array\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python list of categorical values\n",
    "    '''\n",
    "    lst = []\n",
    "    for i in col:\n",
    "        if i <= 27.0:\n",
    "            lst.append('Fast')\n",
    "        else:\n",
    "            lst.append('Slow')\n",
    "    return lst\n",
    "\n",
    "\n",
    "def dataframe_clean(df_file_path):\n",
    "    '''\n",
    "    dataframe_clean: takes in a string and converts\n",
    "    emoticons ':), :(, :-), etc.' with word values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python string with cleaned dataframe\n",
    "    '''\n",
    "    tweets = pd.read_csv(df_file_path)\n",
    "    company_responses = tweets[tweets['inbound'] == False]\n",
    "    # Pick only inbound tweets that aren't in reply to anything...\n",
    "    first_inbound = tweets[pd.isnull(tweets.in_response_to_tweet_id) & tweets.inbound]\n",
    "    # Merge in all tweets in response\n",
    "    inbounds_and_outbounds = pd.merge(first_inbound, tweets, left_on='tweet_id', \n",
    "                                      right_on='in_response_to_tweet_id')\n",
    "    # Filter out cases where reply tweet isn't from company\n",
    "    tweets = inbounds_and_outbounds[inbounds_and_outbounds.inbound_y ^ True]\n",
    "    tweets = tweets.drop(['response_tweet_id_x', 'in_response_to_tweet_id_x', \n",
    "                          'response_tweet_id_y', 'in_response_to_tweet_id_y', 'tweet_id_x','tweet_id_y' ], axis = 1)\n",
    "    # Converts date columns into datetime\n",
    "    tweets.created_at_x = pd.to_datetime(tweets.created_at_x)\n",
    "    tweets.created_at_y = pd.to_datetime(tweets.created_at_y)\n",
    "    # Calculates time_to_respond by subtracting the time between customer and customer support team responses\n",
    "    tweets['time_to_respond'] = tweets.created_at_y - tweets.created_at_x\n",
    "    tweets = tweets.drop(['created_at_y','inbound_x','inbound_y'], axis = 1)\n",
    "    tweets.columns = ['customer_tweet_id', 'time_tweeted', 'customer_tweet_text', 'company_name', 'company_response_text','time_to_respond']\n",
    "    # Calculates which day of the week the tweet was made (Mon-Sun)\n",
    "    tweets.time_tweeted = tweets.time_tweeted.apply(lambda x: day_of_week_num(x))\n",
    "    tweets.time_tweeted = tweets.time_tweeted.apply(lambda x: weekday(x))\n",
    "    tweets.rename({'time_tweeted':'day_tweeted'}, axis = 1, inplace = True)\n",
    "    # Converts thetime to respond into minutes\n",
    "    seconds = tweets['time_to_respond'] / np.timedelta64(1, 's')\n",
    "    tweets.time_to_respond = seconds\n",
    "    tweets.time_to_respond = tweets.time_to_respond.apply(lambda x: math.ceil(x/60))\n",
    "    tweets.rename({'time_to_respond': 'minutes_to_respond'},axis = 1, inplace = True)\n",
    "    # Sets the target values of the dataframe to be above 27 minutes and below or equal to 27 minutes\n",
    "    tweets['Reponse_Speed'] = set_response_category(tweets.minutes_to_respond)\n",
    "    return tweets\n",
    "\n",
    "def lowercase(doc):\n",
    "    '''\n",
    "    lowercase: takes in a string and lowercases it\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python strin converted to lowercase\n",
    "    '''\n",
    "    doc = doc.lower()\n",
    "    return doc\n",
    "\n",
    "\n",
    "def remove_punctuation(doc):\n",
    "    '''\n",
    "    remove_punctuation: takes in a string and \n",
    "    revoes the punctuation via RegEx\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python string with punctuation removed\n",
    "    '''\n",
    "    return re.sub(r'[^\\w\\s]','',doc)\n",
    "\n",
    "\n",
    "\n",
    "def remove_html(doc):\n",
    "    '''\n",
    "    remove_url: takes in a string and removes\n",
    "    html values '<.X?>' using RegEx\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python string with removed html text\n",
    "    '''\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'',doc)\n",
    "\n",
    "def remove_url(doc):\n",
    "    '''\n",
    "    remove_url: takes in a string and removes\n",
    "    url values 'https://, etc.' using RegEx\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python string with removed url text\n",
    "    '''\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r'', doc)\n",
    "\n",
    "def convert_emoticons(doc):\n",
    "    '''\n",
    "    convert_emoticons: takes in a string and converts\n",
    "    emoticons ':), :(, :-), etc.' with word values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python string with converted emoticons\n",
    "    '''\n",
    "    for i in EMOTICONS:\n",
    "        doc = re.sub(u'('+i+')', \"_\".join(EMOTICONS[i].replace(\",\",\"\").split()), doc)\n",
    "    return doc\n",
    "\n",
    "def convert_emojis(doc):\n",
    "    '''\n",
    "    convert_emojis: takes in a string and converts\n",
    "    emojis '😀, 😂, etc.' with word values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python string with converted emojis\n",
    "    '''\n",
    "    doc = emoji.demojize(doc)\n",
    "    doc = re.sub('[:]', '', doc)\n",
    "    return doc\n",
    "\n",
    "def convert_text_speech(doc):\n",
    "    '''\n",
    "    convert_text_speech: takes in a string and converts any\n",
    "    abbreviated text speech, 'ttyl, imo, wtf, etc.' and converts\n",
    "    it into its component words\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python string with converted text speech\n",
    "    '''\n",
    "    text = []\n",
    "    #removes punctuation because that throws off the conversion\n",
    "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    for i in doc.split():\n",
    "        if i.upper() in chat_list:\n",
    "            text.append(chat_dict[i.upper()])\n",
    "        else:\n",
    "            text.append(i)\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    '''\n",
    "    remove_stopwords: takes in a list of strings removes\n",
    "    those strings that contain stopwords imported from the \n",
    "    nlkt.corpus library\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python list with stop words removed\n",
    "    '''\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "    return doc\n",
    "\n",
    "def set_english(df, text_column):\n",
    "    '''\n",
    "    set_english: takes in a string of a text column\n",
    "    outputs the predicted languge of that string to a list\n",
    "    which is used to mask a pandas DataFrame.\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python list with predicted language\n",
    "    '''\n",
    "    identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)  \n",
    "    lst =[]\n",
    "    for i in df[text_column].values: \n",
    "        lang, score = identifier.classify(i)\n",
    "        lst.append(lang)\n",
    "    return lst\n",
    "\n",
    "def lemmatize_words(doc):\n",
    "    '''\n",
    "    lemmatize_words: takes in a list of strings and\n",
    "    outputs the lemmatized version of each word,\n",
    "    lemmatizes based on part of speech.\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python list with lemmatized words\n",
    "    '''\n",
    "    pos_tagged_text = nltk.pos_tag(doc)\n",
    "    doc = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text]\n",
    "    return doc\n",
    "\n",
    "def remove_numbers(doc):\n",
    "    '''\n",
    "    remove_numbers: removes all numbers from a string\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python list with numbers removed\n",
    "    '''\n",
    "    doc = re.sub(r'\\w*\\d\\w*', '', doc).strip()\n",
    "    return doc\n",
    "\n",
    "def remove_non_english_characters(doc):\n",
    "    '''\n",
    "    remove_non_english_characters: takes in a list of strings and\n",
    "    removes all non-english characters, including ASCII.\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python list with non-English characters removed\n",
    "    '''\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "    return doc\n",
    "\n",
    "\n",
    "def preprocessing(doc):\n",
    "    '''\n",
    "    preprocessing: takes in a string and applies \n",
    "    text preprocessing to that string. \n",
    "    (For each function definition, refer to above.)\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: Python string\n",
    "    Returns\n",
    "    -------\n",
    "    doc: Python list with preprocessed text\n",
    "    '''\n",
    "    doc = lowercase(doc)\n",
    "    doc = remove_punctuation(doc)\n",
    "    doc = convert_emoticons(doc)\n",
    "    doc = convert_emojis(doc)\n",
    "    doc = remove_url(doc)\n",
    "    doc = remove_html(doc)\n",
    "    doc = convert_text_speech(doc)\n",
    "    doc = remove_numbers(doc)\n",
    "    doc = remove_non_english_characters(doc)\n",
    "    doc = word_tokenize(doc)\n",
    "    doc = remove_stopwords(doc)\n",
    "    doc = lemmatize_words(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
